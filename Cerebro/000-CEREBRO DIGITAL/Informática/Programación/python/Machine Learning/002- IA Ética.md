---
ID: 2
tags:
  - "#pagmachine_learning"
  - "#python"
nombre: IA √âtica
---
___
# IA √âtica
***La era de los algoritmos. ¬øEnemigos o aliados?***

  

**Introducci√≥n a la IA .Algoritmos. Machine learning**

  

Para hablar de Ia, Algoritmos y machine learning previamente tenemos que mencionar un elemento fundamental. Los datos. La relaci√≥n entre la IA y los datos es muy directa

  

¬øQu√© es un dato?

  

**Tipos de datos:**

  

**Datos personales ‚Äúgen√©ricos‚Äù**. Los datos personales son definidos como la informaci√≥n de cualquier tipo referida a personas determinadas o determinables. Todo aquello que hace que seas Juan y no Pedro.

  

**Datos sensibles**. Los datos ‚Äúsensibles‚Äù conforman una categor√≠a especial. Son aquellos que afectan la esfera √≠ntima del titular y que pueden generar discriminaci√≥n arbitraria. A modo de ejemplo, las normas mencionan, dentro de esta categor√≠a, a aquellos que revelan origen racial, √©tnico, opiniones pol√≠ticas, convicciones religiosas, filos√≥ficas o morales, participaci√≥n o afiliaci√≥n en una organizaci√≥n sindical o pol√≠tica, informaci√≥n referida a la salud, preferencia o vida sexual.

  

**Datos biom√©tricos** Son aquellos obtenidos a partir de un tratamiento t√©cnico espec√≠fico, relativos a las caracter√≠sticas f√≠sicas, fisiol√≥gicas o conductuales de una persona humana, que permitan o confirmen su identidad √∫nica..

  

**Datos gen√©ticos**. Son aquellos relativos a las caracter√≠sticas gen√©ticas heredadas o adquiridas de una persona humana que proporcionen informaci√≥n sobre su f√≠sico o salud, obtenido mediante un an√°lisis de muestra biol√≥gica. Esta categor√≠a no existe en la ley argentina, pero s√≠ se incluye de manera diferenciada en el Proyecto.

  

**Datos de acceso irrestricto**. Existen ciertos datos cuyo tratamiento no requiere consentimiento. En algunas de las normas analizadas, se alude a los siguientes: los que se limiten al nombre y apellido, documento nacional de identidad, identificaci√≥n tributaria y previsional, ocupaci√≥n, fecha de nacimiento, domicilio, correo electr√≥nico, as√≠ como aquellos necesarios para el tratamiento de la informaci√≥n crediticia (24).

  

**TIPOS DE TRATAMIENTO DE DATOS PERSONALES**

  

Los datos personales son sometidos a tratamientos.

  

**Tratamiento**. Las normas, est√°ndares y recomendaciones se refieren al tratamiento

  

en sentido estricto (26), o a cualquier operaci√≥n o procedimiento organizado, electr√≥nico o no, que permita la recolecci√≥n, conservaci√≥n, ordenaci√≥n, almacenamiento, modificaci√≥n, relacionamiento, evaluaci√≥n, bloqueo o destrucci√≥n y todo procesamiento de datos personales en general. Tambi√©n abarca la cesi√≥n de datos personales a trav√©s de comunicaciones, consultas, interconexiones o transferencias.

  

**Tratamiento electr√≥nico**. El tratamiento electr√≥nico o digital que mencionan las normas se vincula con la utilizaci√≥n de tecnolog√≠as de la informaci√≥n y comunicaci√≥n (en adelante, TIC). Aunque las normas en general no diferencian ambos tratamientos en las definiciones, es √∫til distinguir esta categor√≠a, porque ser√° esencial para comprender la diferencia con el tratamiento automatizado. En el tratamiento electr√≥nico, siempre existe un operador humano que, a trav√©s del uso de TIC, efect√∫a la recolecci√≥n, conservaci√≥n, ordenaci√≥n, almacenamiento, modificaci√≥n, relacionamiento, evaluaci√≥n, bloqueo o destrucci√≥n de los datos tratados. **Tratamiento automatizado**. El tratamiento automatizado de datos se encuentra regulado en el Convenio 108 del Consejo de Europa y en el Reglamento 679/2016 de la Uni√≥n Europea. Tambi√©n mencionan este tipo de tratamiento la Ley de Costa Rica, Chile, Per√∫, Panam√°, Uruguay, los Est√°ndares de la Red Iberoamericana y la Ley de Brasil **La diferencia entre el tratamiento automatizado y el tratamiento electr√≥nico se relaciona con la utilizaci√≥n de t√©cnicas de inteligencia artificial.** Especialmente, una de las m√°s sofisticadas, que se conoce como aprendizaje profundo *(deep learning)*, basado en redes neuronales complejas (ampliar estas cuestiones, en *infra* punto II).

  

**¬øQu√© es la IA?** Son sistemas de software ( y en algunos casos tambi√©n de hardware) dise√±ados por seres humanos que, dado u objetivo complejo, act√∫an en la dimensi√≥n f√≠sica o digital mediante la percepci√≥n del entorno a trav√©s de la obtenci√≥n de datos, la interpretaci√≥n de los datos estructurados o no estructurados que recopilan (...) y decidiendo la acci√≥n o acciones √≥ptimas que deben llevar a cabo para lograr el objetivo establecido. Los sistemas de IA pueden (...) adaptar su conducta mediante el an√°lisis del modo en que el entorno se ve afectado por sus acciones anteriores

  

Definici√≥n de principales Capacidades y Disciplinas cient√≠ficas‚Äù, Grupo independiente de expertos de alto nivel sobre inteligencia artificial, abril 2019. **¬øQu√© es un algoritmo?** Conjunto de instrucciones o reglas definidas , ordenadas que permite solucionar un problema, realizar un c√≥mputo, procesar datos y llevar a cabo tareas o actividades, siguiendo pasos sucesivos, llegando a un resultado final. Los algoritmos son la base de la IA, que ejecutan instrucciones a partir de diversas t√©cnicas, para transformar datos en patrones de informaci√≥n, luego en conocimiento y, desde all√≠, automatizar tareas, elaborar predicciones o previsiones.

  

**Machine learning:** El t√©rmino *aprendizaje autom√°tico* se refiere a la detecci√≥n automatizada de patrones significativos en los datos. En las √∫ltimas dos d√©cadas se ha convertido en una herramienta com√∫n en casi cualquier tarea que requiera la extracci√≥n de informaci√≥n de grandes conjuntos de datos. Existen diversas t√©cnicas de machine learning, como aprendizaje supervisado, no supervisado, aprendizaje por refuerzo, y tambi√©n las redes neuronales profundas o deep learning.

  

Los modelos de machine learning son entrenados, cargados de una gran cantidad de datos.Es por ello que para llegar a hablar de sesgos tenemos antes que hablar de datos, y su tratamiento.

  

**Entonces, ¬øC√≥mo relacionamos estos conceptos?**

  

Tenemos‚Üí Grandes cantidades de datos‚Üí estos son procesados con ALGORITMOS de Machine learning‚Üí como resultado ¬†se establecen **patrones.**

  

Ahora bien, una vez dada esta introducci√≥n podemos pasar a nuestro tema principal, que son los Sesgos, y sus posibles soluciones al realizar tratamiento de datos.

  

**SESGOS**

  

**¬øQue es un sesgo? En palabras simples, es omitir considerar informaci√≥n relevante.**

  

Sesgos algor√≠tmicos: Un sistema inform√°tico refleja los valores de los humanos que est√°n implicados en la codificaci√≥n y recolecci√≥n de datos usados para entrenar un algoritmo.

  

FUENTE BASE:[ https://users.dcc.uchile.cl/~rbaeza/bias/sesgos-algoritmos.html ](https://users.dcc.uchile.cl/~rbaeza/bias/sesgos-algoritmos.html)RICARDO BAEZA

  

Otras definiciones:

  

Del ingl√©s BIAS: ‚ÄúLa acci√≥n de apoyar u oponerse a una persona o cosa en particular de manera injusta, debido a que permite que las opiniones personales influyan en su juicio‚Äù (Cambridge English Dictionary).

  

SESGO: Error sistem√°tico en el que se puede incurrir cuando al hacer muestreos o ensayos se seleccionan o favorecen unas respuestas frente a otras‚Äù (RAE)

  

**D√ìNDE PUEDE PRESENTARSE EL SESGO:**

  

- **EN LOS DATOS:**

  

**MUESTRA (CANTIDAD) ‚Äì CALIDAD INCOMPLETOS, INCORRECTOS, INSUFICIENTES**

  

**EL DATO MISMO PUEDE CONTENER UN PREJUICIO - LENGUAJE CONTENIDO EN LOS DATO**

  

- **EN EL ALGORITMO**

  

**DISE√ëO ‚Äì FUNCI√ìN DE √âXITO (resultado)**

  

**SOBRE QU√â SON LOS SESGOS**

  

- 3 TIPOS DE SESGOS CL√ÅSICOS:

¬† - **ESTAD√çSTICO:** C√≥mo obtenemos los datos, de errores de medidas o similares. **EJEMPLO: si la polic√≠a est√° presente en algunos barrios m√°s que en otros, no ser√° extra√±o que la tasa de criminalidad sea m√°s alta donde tenga mayor presencia (o en otras palabras, mediremos m√°s donde est√° uno de los instrumentos de medida).**

- **CULTURAL:** Aquel que deriva de la sociedad, del lenguaje que hablamos o

  

de todo lo que hemos aprendido a lo largo de la vida. **Ejemplo**: **Los estereotipos de las personas de un pa√≠s son un ejemplo claro**

  

- **COGNITIVO:** Aquel que nos identifica y que depende de nuestra

  

personalidad, de nuestros gustos y miedos. **Ejemplo si leemos una noticia que est√° alineada con lo que pensamos, nuestra tendencia ser√° validarla aunque sea falsa.**

  

- DE ESTOS 3 SESGOS (PRINCIPALES) DERIVAN OTROS M√ÅS:

  

**SESGO DE G√âNERO**: No hay suficiente incorporaci√≥n de la teor√≠a ling√º√≠stica feminista en los procesos de ML. Esto se traduce en sesgos en la forma de nombrar; el orden de preferencia; las descripciones sesgadas; el uso y la tipolog√≠a de met√°foras y el grado de presencia/ausencia de las mujeres en los textos escritos. (Susan Leavy, University college dublin)

  

\* https://www.eldiario.es/andalucia/desdeelsur/sesgos-genero-lenguaje-inteligencia-art ificial\_132\_1171463.html

  

**SESGO DE CONFIRMACI√ìN:** La tendencia a favorecer, buscar, interpretar, y recordar, la informaci√≥n que confirma las propias creencias o [hip√≥tesis](https://es.wikipedia.org/wiki/Hip%C3%B3tesis_\(m%C3%A9todo_cient%C3%ADfico\)), dando desproporcionadamente menos consideraci√≥n a posibles alternativas

  

**SESGO DE ORDEN (RANKING):** Se da cuando buscamos en la web, ya que las personas tienden a hacer clics en las primeras posiciones y el buscador podr√≠a interpretar que estas respuestas son mejores que las siguientes.

  

**SESGO DE PRESENTACI√ìN:** Se encuentra en las recomendaciones en el √°mbito del comercio electr√≥nico. Solo aquello que se muestra al usuario podr√° tener clics. Todo lo que no salga en la p√°gina de resultados no puede ser escogido.

  

Esto puede parecer obvio, pero la verdad es que no todos dan cuenta de ello. La √∫nica forma de romper con el ciclo es que se muestra el universo total de resultados (materialmente imposible)

  

El sesgo de presentaci√≥n tiene relaci√≥n con el **FILTRO BURBUJA**: El sistema muestra √∫nicamente aquello que te gusta. Como se basa en las acciones del pasado, no se puede ver lo que se desconoce.

  

**SESGOS DE SEGUNDO ORDEN:** Por ejemplo, cuando una persona usa la informaci√≥n de los primeros resultados de un buscador y la reutiliza para escribir nuevos art√≠culos. Esto significa que cuando la informaci√≥n es recolectada, posiblemente ya est√© sesgada y el buscador crea que son a√∫n m√°s relevantes.

  

\*Existen much√≠simos m√°s (investigaciones han identificado al menos un centenar de sesgos).

  

- **Ahora bien, existe una relaci√≥n de dependencia entre los datos que**

  

**alimentan al algoritmo, y el algoritmo mismo.** En este contexto los sesgos que un modelo puede adquirir en relaci√≥n con los datos con los que es entrenado, ser√≠an al menos los 3 siguientes:

  

(Fuente: Sesgo e Inferencia en redes neuronales ante el derecho. Carlos Amunategui y otros)

  

**Sesgos algor√≠tmicos:** Un sistema inform√°tico refleja los valores de los humanos que est√°n implicados en la codificaci√≥n y recolecci√≥n de datos usados para entrenar un algoritmo.

  

Sesgo algoritmico:

  

**SESGO DE INTERACCI√ìN:** El propio usuario o programador introduce de forma inadvertida un sesgo en el modelo por la manera en que interact√∫a con √©l.

  

**SESGO LATENTE:** Cuando el modelo realiza correlaciones inapropiadas, generalmente al establecer falsos nexos entre puntos de datos.

  

**SESGO DE SELECCI√ìN:** Cuando la base de datos no es suficientemente representativa de la diversidad existente en el medio social.

  

**Importante:**

  

**EL SESGO EN EL APRENDIZAJE AUTOM√ÅTICO SE PUEDE DETECTAR Y DISMINUIR CON BASTANTE FACILIDAD VS. SESGOS EN HUMANOS**

  

NECESIDAD DE INVERTIR EN HERRAMIENTAS DE EVALUACI√ìN, CONTROL Y MITIGACI√ìN.

  

Al decir que los algoritmos presentan sesgos, estamos trasladando la responsabilidad a los sistemas de inteligencia artificial, cuando en realidad, son las personas tras los sistemas, quienes deben preocuparse y en consecuencia, ocuparse de evaluar, controlar y mitigar resultados que pueden potencialmente generar da√±os.

  

**CASOS CONTROVERSIALES: DISCRIMINACI√ìN ALGOR√çTMICA**

  

Si tomamos por un lado lo que es la IA explicado inicialmente y la existencia de m√∫ltiples sesgos (cognitivos y de IA) por otro, obtenemos como resultado, el riesgo (existente) de que sistemas basados en ML tomen decisiones que reproduzcan e intensifiquen discriminaciones y en consecuencia, se perpet√∫e una sociedad en que abundan las injusticias.

  

Basta una r√°pida b√∫squeda en Google, introducir las palabras ‚ÄúIA‚Äù y ‚ÄúSesgos‚Äù para pensar que la situaci√≥n se est√° saliendo de control. Hay much√≠simos casos controversiales y la repercusi√≥n y el esc√°ndalo a su alrededor es en cierto modo leg√≠tima, por cuanto se han vulnerado personas y sus derechos, y han sido decisiones que han cambiado vidas, afectando por lo general, a grupos m√°s vulnerables.

  

Por mencionar algunos casos:

  

**Caso Amazon ‚Äì 2014**: Agente de selecci√≥n de personal. En 2014, la empresa tecnol√≥gica quiso automatizar su selecci√≥n de curr√≠culos a fin de confeccionar una lista corta de candidatos a un puesto determinado. Para ello construy√≥ un algoritmo que se basaba en la informaci√≥n de los empleados que la firma ya ten√≠a, su tasa de retenci√≥n y ascensos. Puesto que la mayor parte de los empleados de Amazon eran hombres, el algoritmo procedi√≥ a eliminar de la lista de candidatos a las mujeres, construyendo listas cortas exclusivamente masculinas. Lo m√°s inquietante es que en los curr√≠culos no se inclu√≠a la menci√≥n del sexo del solicitante, pero, aparentemente, el agente lo infiri√≥ de otros datos incluidos en el curr√≠culo. Como el algoritmo no pudo ser corregido para lograr la objetividad inicialmente deseada, la compa√±√≠a debi√≥ retirarlo.

  

**Caso Google ‚Äì 2015:** La aplicaci√≥n de reconocimiento facial Google Photos, etiquet√≥ una de las fotos de Jacky Alcine, ciudadana afroamericana, con la palabra ‚Äúgorila‚Äù.

  

Esto sucedi√≥ porque el algoritmo no hab√≠a sido entrenado con suficientes im√°genes de personas de piel oscura.

  

**Caso Microsoft ‚Äì 2016:** ‚ÄúTay‚Äù, era un *chatbot* cuyo fin era imitar el comportamiento de una adolescente curiosa y entablar en las redes sociales una conversaci√≥n informal y divertida con una audiencia de entre 18 y 24 a√±os. El proyecto mostrar√≠a las promesas y el potencial de las interfaces conversacionales alimentadas por inteligencia artificial. Sin embargo, en menos de 24 horas, el ‚Äúinocente‚Äù Tay a trav√©s de *tweets*, mostraba su empat√≠a hacia Hitler o su apoyo al genocidio al responder a preguntas de los usuarios de las redes sociales, son algunos ejemplos, adem√°s de insultos raciales y comentarios sexistas y hom√≥fobos. Tambi√©n defendi√≥ el Holocausto, los campos de concentraci√≥n o la supremac√≠a blanca, y se mostr√≥ contraria al feminismo.

  

**Caso Universidades UK ‚Äì 2020:** Con la pandemia del Covid-19, las universidades de UK optaron por buscar una alternativa a los ex√°menes presenciales de admisi√≥n.

  

Estudios sobre los puntajes anteriores hab√≠an demostrado la existencia de sesgos en funci√≥n de la edad, g√©nero y etnia, siendo entonces la equidad una cuesti√≥n de especial importancia a combatir. Teniendo eso en mente, las autoridades resolvieron utilizar un algoritmo.

  

[Casi el 40% de los estudiantes](https://www.theguardian.com/education/2020/aug/07/a-level-result-predictions-to-be-downgraded-england) terminaron recibiendo puntajes de ex√°menes rebajados de las predicciones de sus maestros, lo que amenaz√≥ con costarles sus lugares universitarios. [El an√°lisis](https://www.theguardian.com/education/2020/aug/13/england-a-level-downgrades-hit-pupils-from-disadvantaged-areas-hardest) del algoritmo tambi√©n revel√≥ que hab√≠a da√±ado de manera desproporcionada a los estudiantes de la clase trabajadora y comunidades desfavorecidas e inflado las puntuaciones de los estudiantes de las escuelas privadas.

  

Finalmente, se revoc√≥ la decisi√≥n de utilizar el algoritmo como elemento decidor de los procesos de admisi√≥n y los estudiantes ahora recibir√°n las puntuaciones previstas por su profesor o por el algoritmo, la que sea m√°s alta.

  

As√≠ entonces, no pretendemos desconocer que los algoritmos utilizados en el mercado laboral, bancos, compa√±√≠as de seguros, entidades comerciales, sistemas educativos, etc. est√©n provistos de determinados valores e ideolog√≠as en su ADN. Sin embargo, queremos centrar nuestra presentaci√≥n en las posibles soluciones que se est√°n desarrollando hoy a estos problemas.

  

Ciertamente no se le puede exigir objetividad y justicia a las entidades comerciales, a los bancos o a las compa√±√≠as de seguro. Hay en ellos una intenci√≥n de lucrar y de obtener la mayor ganancia posible a cualquier costo. Distinto es el caso de las redes de empleabilidad, de los sistemas de educaci√≥n, de ayuda social y de justicia. En ellos debe existir una garant√≠a de equidad.

  

**SESGO HUMANO VS. SESGO ALGOR√çTMICO: S. MULLAINATHAN**

  

Es importante abrir los ojos y entender que los sesgos no son creados por la IA, y que no podr√°n simplemente desaparecer. Se requiere un cambio m√°s profundo a nivel de estructuras, sistemas y de sociedad.

  

A pesar de lo catastr√≥fico que han resultado casos como los mencionados y otros tanto m√°s, pareciera que los sistemas de IA cuentan con la virtud de ser m√°s flexibles y aprender mejor y m√°s r√°pido de sus errores, en comparaci√≥n con nosotros los humanos. Los sesgos en la IA ser√≠an m√°s f√°ciles de corregir, que los sesgos que tenemos nosotros los humanos.

  

El a√±o 2004 el Marianne Bertrand y Sendhil Mullainathan publicaron un estudio llamado: *Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination.* En Diciembre del 2019 ese estudio adquiri√≥ nueva relevancia.

  

La primera publicaci√≥n (2004) dio cuenta de un experimento en el cual se midi√≥ la discriminaci√≥n racial en el campo laboral. Se respondi√≥ a diversas ofertas de trabajo con CVs ficticios. A cada CV se le asign√≥ aleatoriamente un nombre que sonaba ‚Äúmuy afroamericano‚Äù o ‚Äúmuy blanco‚Äù . Los resultados mostraron una discrminaci√≥n significativa contra los nombres afroamericanos, obten√≠as menos entrevistas de trabajo.

  

El a√±o 2019 Mullainathan realiz√≥ otro experimento, en el cual tambi√©n se buscaba medir la discriminaci√≥n racial. Dos pacientes buscaban atenci√≥n m√©dica. Ambos estaban lidiando con la diabetes y la presi√≥n arterial alta. Un paciente era negro, el otro era blanco. ¬†El primer paciente recibi√≥ una atenci√≥n peor.

  

Las conclusiones son las mismas, sin embargo existe una gran diferencia entre una investigaci√≥n y otra, el a√±o 2004 fueron personas encargadas de los procesos de contrataci√≥n quienes tomaron las decisiones. El a√±o 2019, fue un programa de computadora quien asign√≥ determinado puntaje para acceder a servicio de salud.

  

Entre uno y otro estudio puede observarse la existencia de sesgos, humanos por un lado, del algoritmo por otro. Pero otra gran diferencia es lo que se requiere para develar y corregir la existencia de ese sesgo.

  

Identificar el comportamiento discriminatorio de un grupo particular de personas (gerentes de contrataci√≥n) suele ser muy dif√≠cil. Por el contrario, descubrir la discriminaci√≥n algor√≠tmica fue mucho m√°s sencillo. ‚Äú*Este fue un ejercicio estad√≠stico, el equivalente a preguntar al algoritmo "¬øqu√© har√≠as con este paciente?" cientos de miles de veces y trazando las diferencias raciales. El trabajo era t√©cnico y rutinario, y no requer√≠a ni sigilo ni ingenio‚Äù. (Cita: Art√≠culo NY: <https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html>)*

  

Los seres humanos son inescrutables de una manera que los algoritmos no lo son. Nuestras explicaciones de nuestro comportamiento est√°n cambiando y construidas despu√©s de los hechos. Para medir la discriminaci√≥n racial por parte de las personas, debemos crear circunstancias controladas en el mundo real donde solo la raza difiere. Para un algoritmo, podemos crear igualmente controlados simplemente aliment√°ndolo con los datos correctos y observando su comportamiento. <https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html>

  

**¬øEs la inteligencia artificial un enemigo?¬øC√≥mo podemos hacer de la inteligencia artificial nuestro aliado?**

  

La **red iberoamericana de protecci√≥n de datos personales**, en la redacci√≥n de las Orientaciones especificas para el cumplimiento de los principios rectores de la protecci√≥n de datos personales establece:

  

-Que el modelo de la IA no debe enfatizar la informaci√≥n relacionada con el origen racial o √©tnico, opini√≥n pol√≠tica, religi√≥n, orientaci√≥n sexual, y adem√°s deja sin dudas que se debe ESTABLECER un sistema de monitoreo constante del modelo de IA con la finalidad de identificar la existencia de sesgos y en la medida de lo posible implementar una gesti√≥n de riesgos, obteniendo como producto final reportes y estadisticas que permitan analizar los resultados. ¬†(datos sensibles)

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.001.png|1500]]

  

Los datos de entrada est√°n afectados por dos variables en principio: los sesgos (incorporaci√≥n de datos parciales, insuficientes, no actualizados o manipulados) y la pertinencia (relevancia, inconsistencia o completitud de datos). El desarrollo del algoritmo, se puede ver afectado por los patrones (sesgos de la l√≥gica de programaci√≥n, inclusi√≥n de funciones utilizadas para su codificaci√≥n) y los errores (condiciones de la operaci√≥n que reflejan un funcionamiento diferente al previsto y atentan contra las premisas del dise√±o. Por √∫ltimo los riesgos est√°n relacionados con la pertinencia y precisi√≥n de los resultados del algoritmo y como respuesta al an√°lisis de datos de entrada.

  

**POSIBLES SOLUCIONES**

  

Sugerimos dividir las posibles soluciones en **√ÅREA TECNICA** (es decir soluciones para el sistema mismo) y **HABILIDADES HUMANAS O EDUCACI√ìN √âTICA** (teniendo en cuenta que los sistemas son realizados por humanos, al menos por el momento, y que factores como la calidad del dato recolectado, es decir las caracter√≠sticas pueden ser menos informativas o recopiladas de manera menos confiable para ciertas partes de la poblaci√≥n, los sesgos que tenemos los seres humanos como inherentes a la existencia misma, influyen de manera directa en los sistemas que se desarrollan)

  

**-AREA T√âCNICA:**

  

**¬øC√≥mo aprenden a discriminar los modelos?** https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de

  

-muestra sesgada -ejemplos contaminados

  

-caracter√≠sticas limitadas

  

-disparidad del tama√±o de la muestra

  

Un conjunto de caracter√≠sticas que admite precisi√≥n para el grupo mayoritario puede no serlo para un grupo minoritario.

  

Diferentes modelos con la misma precisi√≥n informada pueden tener una distribuci√≥n muy diferente entre la poblaci√≥n.

  

¬øC√≥mo podemos solucionarlo?

  

**Modelo Fair ML**

  

Fairlearn es un nuevo paquete de Python desarrollado por Microsoft. Implementa varios algoritmos para detectar y mitigar problemas de equidad de grupo en modelos de aprendizaje autom√°tico.

  

¬øC√≥mo funciona?

  

Evaluaci√≥n de la equidad: Fairlearn contiene un Fairlearn Dashboard componente y un conjunto de m√©tricas que lo ayudan a medir la equidad de su modelo.

  

Mitigar la injusticia: junto con las m√©tricas y el tablero, hay un conjunto de algoritmos fairlearn para ayudar a mitigar el comportamiento injusto en los modelos. Puede, por ejemplo, utilizar un algoritmo de posprocesamiento para mejorar los modelos existentes. Pero tambi√©n hay un par de algoritmos que te ayudan a mejorar el modelo durante el entrenamiento.

  

El paquete fairlearn contiene un componente llamado FairlearnDashboard. Es un widget que podemos usar dentro de un cuaderno de Python que mide y visualiza dos m√©tricas para nuestro modelo:

  

¬øC√≥mo puedo utilizar fairlearn para mejorar un modelo injusto?

  

El paquete fairlearn contiene varios algoritmos que ayudan a resolver la injusticia en los modelos sin cambiar los datos que usamos para entrenar el modelo.

  

Hay dos estrategias que podemos aplicar para resolver la injusticia con fairlearn:

  

Para los modelos existentes, podemos mitigar la injusticia mediante el posprocesamiento.

  

Para los nuevos modelos, podemos utilizar algoritmos de reducci√≥n para mejorar la equidad.

  

Usar el algoritmo ThresholdOptimizer para mejorar un modelo existente TresholdOptimizer se basa en un documento llamado "Igualdad de oportunidades en el aprendizaje supervisado". Intenta corregir el modelo para que ya no discrimine a grupos espec√≠ficos de usuarios, bas√°ndose en un conjunto de caracter√≠sticas sensibles.https://fairlearn.github.io/

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.002.jpeg|1500]]

  

**Modelo de Open data institute**

  

Este es un modelo donde el programador ¬†o desarrollador va respondiendo preguntas y al final obtiene un resultado que dice si el algoritmo tiene o tendr√° un impacto positivo o no.

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.003.jpeg|1000]]

  

**AEQUITAS**

  

Fuentes:

  

Pedro Saleiro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, Rayid Ghani, Aequitas: A Bias and Fairness Audit Toolkit, arXiv preprint arXiv:1811.05577 (2018). (PDF)

  

Charla Gob Lab UAI: Analizando disparidades en modelos de ML (youtube) <http://www.datasciencepublicpolicy.org/projects/aequitas/>

  

Es una herramienta de c√≥digo abierto que permite auditar modelos de ML para detectar y mitigar sesgos.

  

Permite a los usuarios probar modelos para medir distintas m√©tricas de sesgo y definiciones de equidad (fairness) en relaci√≥n con m√∫ltiples grupos y subgrupos de la poblaci√≥n, con el objetivo de crear consciencia entre los distintos grupos de inter√©s (stakeholders) respecto a la existencia de bias y de fairness (justicia) como un KPI principal (indicador).

  

\*\*Un KPI, conocido tambi√©n como indicador clave o medidor de desempe√±o o indicador clave de rendimiento, es una medida del nivel del rendimiento de un proceso. El valor del indicador est√° directamente relacionado con un objetivo fijado previamente y normalmente se expresa en valores porcentuales

  

La posibilidad de medir, permite optimizar el sistema y habilitar el uso de m√©todos de mitigaci√≥n de sesgo o escoger mejor modelos de ML.

  

La idea es que Aequitas sea una ***herramienta*** de f√°cil uso para que los cient√≠ficos de datos y los tomadores de decisiones no tengan excusa para no medir el impacto de los m

  

Existen 3 maneras de usarlo:

  

- Desde el sitio web y subir el dataset (no queda guardado)

- Directamente desde una biblioteca de Python

- Command Line tool (l√≠nea de comandos)

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.004.jpeg|1200]]

  

Desde Triage (\*)

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.005.png|700]]

  

- Definir atributos que consideran importantes (que no se quieren perjudicar o proteger): Age - Sex - Race

- Cu√°l es el grupo de referencia (favorecido): age: 35-49; sex: male; race: white

  

Desde Python.

  

Interfaz orientada a objetos. 3 simples pasos:

  

- Define los grupos . Data frame. Se calcula cantidades estad√≠sticas para cada subgrupo

- Calcula las disparidades entre las m√©tricas, toma razones entre las m√©tricas y el bias

- fairness, da reporte sencillo de qu√© cosas no est√°n siendo justas en el data set

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.005.png|700]]

  

En forma **extremadamente simplificada**:

  

- Definir grupo (data frame).

- Se sube el dataset, las predicciones.Se configura la m√©trica de sesgo para grupos de atributos de inter√©s protegidos, as√≠ como grupos de referencia.

- Se generar√° un reporte con las m√©tricas.

- Fairness: Luego arrojar√° en qu√© m√©tricas hay disparidad seg√∫n los grupos definidos como protegidos y grupos bases.

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.006.jpeg|1200]]

  

Fuente: <http://aequitas.dssg.io/>

  

**QU√â SE NECESITA PARA AUDITAR UN MODELO?**

  

PREDICCIONES

  

Atributos que definen a grupos protegidos. Ej. Etnicidad, sexo, nivel de ingresos

  

Etiquetas reales. Si hay errores en disparidad. si esta persona estuvo o no en tal escuela, si fallo en ella etc.

  

**El paradigma de la auditoria: *Si no lo puedes medir, no lo puedes mejorar***

  

Se promueve el uso de Aequitas por dos grupos de actores diferentes, por el cient√≠fico de datos a la hora de desarrollar modelos de IA y medir el impacto de riesgo (los cient√≠ficos estar√≠an acostumbrados a seleccionar en base al performance o desempe√±o del modelo, pero debiera ser en base a qu√© tan justo es). Y por los tomadores de decisiones, para que antes de aceptar aplicar un modelo de IA o si ya lo utilizan, poder comprender qu√© sesgos existen en ese modelo y mitigarlos de ser necesario.

  

Aequitas in the larger context of the ML pipeline. Audits must be carried internally by data scientists before evaluation and model selection. Policymakers (or clients) must audit externally before accepting a model in production as well as perform periodic audits to detect any fairness degradation over time.Audits must be carried internally by data scientists before evaluation and model selection. Policymakers (or clients) must audit externally before accepting a model in production as well as perform periodic audits to detect any fairness degradation over time.

  

![[Aspose.Words.445e149a-021f-470e-84fd-67218774968d.007.jpeg|1200]]

  

Audits must be carried internally by data scientists before evaluation and model selection.![](Aspose.Words.445e149a-021f-470e-84fd-67218774968d.008.png) Policymakers (or clients) must audit externally before accepting a model in production as well as perform periodic audits to detect any fairness degradation over time.

  

Auditar para bias y fairness es el primer paso para crear conciencia y tomar decisiones m√°s informadas respecto al desarrollo y puesta en producci√≥n de modelos de ML, que pueden afectar la vida de los individuos.

  

Aequitas permite realizar an√°lisis de sesgos y equidad respecto de m√∫ltiples atributos y no solo respeto de uno solo pre definido. **Se basa en una definici√≥n de sesgo entendida como medida de disparidad entre grupos, en comparaci√≥n con un grupo de referencia.** A su vez, el grupo de referencia, se puede seleccionar utilizando varios criterios. *Our formulation, allows performing bias and fairness analysis on any multi-valued attribute, and not just for pre-defined protected attributes. We define bias as a disparity measure across groups when compared with a reference group. This reference group can be selected using different criteria.*

  

Se considera que hay paridad en el impacto, cuando la fracci√≥n de elementos del grupo que se predice como positiva, es la misma en todos los grupos. Por otro lado, se considera que hay paridad estad√≠stica o demogr√°ficamente en los *predictor*, si hay una fracci√≥n igual de elementos de cada grupo entre todos los positivos previstos (es decir, se distribuyen todos por igual).

  

Por lo tanto, definimos el sesgo como una medida de disparidad de los valores m√©tricos de un grupo dado en comparaci√≥n con un grupo de referencia. Esta referencia se puede seleccionar utilizando diferentes criterios.

  

Por ejemplo, se podr√≠a usar el grupo mayoritario (con mayor tama√±o) entre los grupos definidos por A, o el grupo con el m√≠nimo valor de la m√©trica de grupo, o el enfoque tradicional de fijar un grupo hist√≥ricamente favorecido (p. ej. raza: blanca).

  

Las diferentes medidas de equidad var√≠an en importancia para el usuario final seg√∫n el costo e impacto de la intervenci√≥n:

  

- Si las intervenciones son muy caras o pueden perjudicar al individuos, entonces querr√≠amos minimizar los falsos positivos (centr√°ndonos en la tasa de descubrimiento falso y / o Tasa de falsos positivos).

- Si las intervenciones son predominantemente asistenciales, deber√≠a preocuparse m√°s por los falsos negativos (centr√°ndose en la tasa de omisiones falsas y / o la tasa de falsos negativos).

- Falsos positivos

- Falsos negativos

  

Lo distintivo de Aequitas es que tiene presente que bias y fairness no son conceptos absolutos y que est√°n necesariamente vinculados al escenario al que se aplican, como asimismo al an√°lisis y a la interpretaci√≥n dentro de ese contexto. Por otro lado, pretende ser √∫til tambi√©n a autoridades en general y no limitarse a ser entendido √∫nicamente por personas con conocimiento t√©cnico

  

El criterio de equidad/justicia (fairness) es flexible ya que se basa en un par√°metro de valor real para controlar el rango de valores de disparidad que pueden considerarse justos.

  

Definen dos tipos de injusticia: Supervisada y no supervisada (unsupervised).

  

El concepto de equidad de Aequitas se basa en un impacto de grupo utilizando restricciones de paridad. Nuestra formulaci√≥n e implementaci√≥n

  

de justicia es flexible, ya que se basa en un par√°metro de valor real.

  

**ARQUITECTURA**. Se basa en los siguientes componentes:

  

Input data

  

Requiere la siguiente data como input:

  

- Set de predicciones: entidades/grupos? y puntajes dados a esas entidades

- atributos para cada entidad (edad, sexo, etc)

- Resultados/etiquetas para cada entidad

  

Par√°metros de configuraci√≥n

  

- Atributos y valores de inter√©s (ej. G√©nero: masculino, femenino). Definir qu√© atributos se consideran importantes y cu√°l es el grupo de referencia sobre el cual trabajar

- Valores de referencia para cada grupo, para calcular las proporciones de sesgo (ej. Masculino para g√©nero)

- Medidas de sesgo para calcular. La disparidad no ser√° un valor absoluto, depender√° de qu√© m√©trica y performance se usar√°.

  

Output

  

Aequitas generar√° outputs en los siguientes formatos:

  

- Gr√°fico de database: C√°lculo de los errores en la m√©trica y la medida de

  

disparidad.

  

- Reporte en PDF.

- Reporte visual interactivo: Permite al usuario explorar en forma interactiva

  

**Posibles soluciones desde el √°mbito Humano**

  

Cualquier tipo de protocolo o regulaci√≥n es necesario que se considere a los siguientes actores: Sector publico, Sector privado, grandes empresas- pymes- participaci√≥n ciudadana

  

Sector publico (Estado y organismos internacionales)

  

-Gobernanza que cree un marco para la cooperaci√≥n de las autoridades competentes.

  

-Marco Juridico que sirva de sustento y protecci√≥n ante los avances tecnol√≥gicos En el ¬†libro blanco del 2020 establece: La estructura de gobernanza debe garantizar la mayor participaci√≥n de partes interesadas posible. Debe consultarse a las partes interesadas (organizaciones de consumidores e interlocutores sociales,

  

empresas, investigadores y organizaciones de la sociedad civil) sobre la aplicaci√≥n y futuro desarrollo del marco.

  

Respecto de las actividades tecnol√≥gicas, creemos necesario adem√°s que existan c√≥digos de √©tica y que los mismos sean presentados desde la formaci√≥n terciaria o universitaria.

  

Particulares (desarrolladores) Interdisciplinariedad

  

Junto con la recolecci√≥n de mejores datos creemos que la interdisciplinariedad es sumamente necesaria. Con filtrado colaborativo, es decir, agregando personas o productos al equipo que no sean afines a nosotros al momento de ense√±ar a un algoritmo.

  

**ALGUNAS POSIBLES CONCLUSIONES:**

  

- [**En un informe](https://www.oii.ox.ac.uk/blog/ai-work-overcoming-structural-challenges-to-ensure-successful-implementation-of-ai-in-the-workplace/) **publicado la semana pasada (fines de agosto) por el Instituto de Internet de Oxford, los investigadores encontraron que una de las trampas m√°s comunes en las que caen las organizaciones al implementar algoritmos es la creencia de que solucionar√°n problemas estructurales realmente complejos. Estos proyectos "se prestan a una especie de pensamiento m√°gico", dice Gina Neff, profesora asociada del instituto y coautora del informe. Los algoritmos no pueden reparar sistemas rotos. Heredan los defectos de los sistemas en los que est√°n ubicados.**

- Los sesgos y las decisiones basadas en sesgos siempre han existido. Los sesgos podr√°n provenir de los humanos, de los datos (muestra y etiquetado), de los sistemas de inteligencia artificial tanto de las decisiones basadas en los sistemas de ML, como de las acciones posteriores basadas en esas decisiones. (ML: decisiones; acciones)

- Kate Crawford, de Microsoft Research, explicaba en [IEEE Spectrum](https://spectrum.ieee.org/tech-talk/tech-history/dawn-of-electronics/untold-history-of-ai-the-birth-of-machine-bias) que "es hora de reconocer que los algoritmos son una creaci√≥n humana que hereda nuestros prejuicios [...]: **nuestra IA ser√° tan buena como lo seamos nosotros".**

- Importancia del factor humano, la √©tica y los valores defendemos y queremos

- **La definici√≥n de equidad no depender√° de los cient√≠ficos de datos, tampoco de las estad√≠sticas, depender√° del contexto en el cual se inserte el sistema. De ah√≠ entonces es que cobra vital importancia el factor humano, la √©tica y los valores que defendemos y queremos, por un lado, y la necesidad de monitorear constantemente los sistemas de inteligencia artificial por otro. En este sentido, es imperativo que la implementaci√≥n de sistemas de IA vaya acompa√±ada de auditor√≠as a los modelos, como alguno de los que mencionamos previamente.**

¬† - **A pesar de que revisar el concepto de equidad y sus par√°metros en el algoritmo antes de implementar un sistema de ML, puede parecer de una necesidad evidente y obvia, a√∫n no es un procedimiento estandarizado. Por eso es importante dar a conocer los desarrollos que**

  

**se est√°n llevando a cabo por aquellos que creen en una IA √∫til y responsable.**

  

- No existe una definici√≥n universalmente aceptada y absoluta de lo que significa que decisiones basadas en un sistema un sistema sean justa

- **Si bien muchos de los modelos de IA hoy utilizado contienen sesgos y han llevado a decisiones discriminatorias, investigaciones han demostrado que las decisiones de los *policy maker* parecieran ser m√°s sesgadas a√∫n y por sobretodo m√°s dif√≠ciles de corregir, b√°sicamente por su car√°cter de seres humanos.** Los sistemas tienden a ser m√°s precisos, con igual o menos sesgos que las personas, pero m√°s ‚Äúf√°cilmente‚Äù corregibles.

- Los modelos no se insertan en el vac√≠o, son sist√©micos e involucran m√∫ltiples factores y variantes, por tanto, puede que sean justos,pero el resultado de las acciones que habilita el modelo de ML no lo sea. Importancia de entender el contexto de los sistemas de IA, y del resultado de los mismo.

  

**\* INFORMACI√ìN DE INTER√âS SOBRE OTROS SISTEMAS DE AUDITOR√çA A**

  

**SISTEMAS DE ALGOR√çTMICOS y otra info de relevancia**

  

[https://www.mckinsey.com/business-functions/mckinsey-analytics/how-we-help-client s](https://www.mckinsey.com/business-functions/mckinsey-analytics/how-we-help-clients)

  

<https://algorithmwatch.org/en/jobs/>

  

https://www.technologyreview.es/s/7950/unamonos-para-evitar-la-discriminacion-de-l os-algoritmos-que-nos-gobiernan

  

https://orcaarisk.com/

  

https://www.technologyreview.es/s/8344/los-algoritmos-sesgados-estan-por-todas-pa rtes-y-parece-que-nadie-le-importa

  

https://ainowinstitute.org/AI\_Now\_2019\_Report.pdf https://revista.une.org/11/la-eliminacion-de-los-sesgos-en-los-algoritmos.html

  

[ref1]: Aspose.Words.445e149a-021f-470e-84fd-67218774968d.005.png


___

%%
tags: #pagmachine_learning #python   

V√≠nculos: [[000-Men√∫ Machine Learning üìÉ|Men√∫ Machine Learning üìÉ]] 
%%