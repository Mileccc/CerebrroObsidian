---
ID: 10
"tags:": 
nombre: Ejemplo de Embeddings
---
___

# Ejemplo de Embeddings

![](https://youtu.be/Wxj0u0vqUk0?t=4493)
___

## Embedding de personalidad: Â¿CÃ³mo somos?

En una escala de 0 a 100, Â¿QuÃ© tan introvertido/extrovertido eres (donde 0 es el mÃ¡s introvertido y 100 es el mÃ¡s extrovertido)? Â¿Alguna vez ha realizado una prueba de personalidad como la prueba de los Cinco Grandes Rasgos de la Personalidad? Si no lo has hecho, estas son pruebas que te hacen una lista de preguntas, y luego te califican en una serie de ejes, siendo la introversiÃ³n/extraversiÃ³n uno de ellos.

Imaginemos que Jay obtuvo una puntuaciÃ³n de 38/100 en el rasgo introversiÃ³n/extraversiÃ³n. Podemos graficar eso de esta manera:

![[introversion-extraversion-100.png|600]]

Supongamos que, ademÃ¡s de ese, tambiÃ©n contemplamos otro de los 5 rasgos y que las escalas de 0 a 100 son reemplazadas por una escala de -1 a 1. AsÃ­ obtendrÃ­amos un vector que represente a Jay de la siguiente manera:

![[two-traits-vector.png|600]]

Ahora podemos decir que este vector representa parcialmente la personalidad de Jay. La utilidad de tal representaciÃ³n viene cuando compararlo con otras dos personas. Digamos que lo atropella un autobÃºs y necesita ser reemplazado por alguien con una personalidad similar. En la siguiente figura, Â¿CuÃ¡l de las dos personas se parece mÃ¡s a Jay?

![[personality-two-persons.png|600]]

Podemos ver que la persona nÂ° 1 es mÃ¡s parecida a Jay porque grÃ¡ficamente su vector estÃ¡ mÃ¡s "cerca" al de Jay. Ahora supongamos que tenemos un vector con los 5 rasgos... el anÃ¡lisis grÃ¡fico no es una opciÃ³n. Sin embargo, podemos calcular la similaridad a travÃ©s del Ã¡ngulo entre los vectores (que se calcula utilizando el coseno como ya vimos) para obtener una mÃ©trica que facilite la comparaciÃ³n.

![[embeddings-cosine-personality.png|800]]

Seguimos viendo que la persona mÃ¡s parecida es la nÂ°1, pero ahora tenemos una mÃ©trica para saber cuÃ¡n mejor reemplazo es con respecto a la otra persona.

## Embedding de palabras: Â¿CÃ³mo codificamos el significado?
Para que las palabras sean procesadas por modelos de aprendizaje automÃ¡tico, necesitan alguna forma de representaciÃ³n numÃ©rica que los modelos puedan usar en sus cÃ¡lculos. Ahora que entendemos el poder de representaciÃ³n de los vectores, podemos proceder a observar ejemplos de vectores de palabras entrenados (tambiÃ©n llamados embeddings de palabras) y comenzar a observar algunas de sus propiedades interesantes.

Word2Vec fue el primer algoritmo en demostrar que podemos usar un vector para representar correctamente las palabras de una manera que capturara las relaciones semÃ¡nticas o de significado. Por ejemplo, al analizar estos vectores podemos saber si las palabras son similares u opuestas (analizando la direcciÃ³n y el sentido del vector), o que un par de palabras como â€œEstocolmoâ€ y â€œSueciaâ€ tienen entre ellas la misma relaciÃ³n que tienen â€œEl Cairoâ€ y â€œEgiptoâ€ (analizando las distancias entre los vectores), o tambiÃ©n relaciones sintÃ¡cticas o basadas en la gramÃ¡tica (por ejemplo, la relaciÃ³n entre â€œtenÃ­aâ€ y â€œtengoâ€ es la misma que entre â€œeraâ€ y â€œesâ€).

Este es un embedding para la palabra "rey" (vector GloVe entrenado en Wikipedia):

```
0.50451,0.68607,âˆ’0.59517,âˆ’0.022801,0.60046,âˆ’0.13498,âˆ’0.08813,0.47377,âˆ’0.61798,âˆ’0.31012,âˆ’0.076666,1.493,âˆ’0.034189,âˆ’0.98173,0.68229,0.81722,âˆ’0.51874,âˆ’0.31503,âˆ’0.55809,0.66421,0.1961,âˆ’0.13495,âˆ’0.11476,âˆ’0.30344,0.41177,âˆ’2.223,âˆ’1.0756,âˆ’1.0783,âˆ’0.34354,0.33505,1.9927,âˆ’0.04234,âˆ’0.64319,0.71125,0.49159,0.16754,0.34344,âˆ’0.25663,âˆ’0.8523,0.1661,0.40102,1.1685,âˆ’1.0137,âˆ’0.21585,âˆ’0.15155,0.78321,âˆ’0.91241,âˆ’1.6106,âˆ’0.64426,âˆ’0.51042
```
---

Es una lista de 50 nÃºmeros. No podemos decir mucho mirando los valores. Pero visualicÃ©moslo un poco para poder compararlo con otros vectores de palabras. Pongamos todos estos nÃºmeros en una fila y codifiquemos por colores las celdas segÃºn sus valores (rojo si estÃ¡n cerca de 2, blanco si estÃ¡n cerca de 0, azul si estÃ¡n cerca de -2):

![[king-colored-embedding.png|1700]]

Procederemos ignorando los nÃºmeros y solo observando los colores para indicar los valores de las celdas. Ahora comparemos "Rey" con otras palabras:

![[Sin tÃ­tulo.png|800]]

Algunas cosas para seÃ±alar:

- Hay una columna roja recta a travÃ©s de todas estas palabras diferentes. Son similares a lo largo de esa dimensiÃ³n (y no sabemos quÃ© codifica cada dimensiÃ³n)
- Puedes ver cÃ³mo "mujer" y "niÃ±a" son similares entre sÃ­ en muchos lugares. Lo mismo con "hombre" y "niÃ±o"
- â€œniÃ±oâ€ y â€œniÃ±aâ€ tambiÃ©n tienen lugares donde son similares entre sÃ­, pero diferentes de â€œmujerâ€ u â€œhombreâ€. Â¿PodrÃ­an estar codificando una vaga concepciÃ³n de la juventud?
- Todas menos la Ãºltima palabra son palabras que representan personas. Podemos, por ejemplo, ver que la columna azul va hacia abajo y se detiene antes del embedding de "agua".
- Hay lugares claros donde "rey" y "reina" son similares entre sÃ­ y distintos de todos los demÃ¡s. Â¿PodrÃ­an estar codificando un concepto vago de realeza?

Otro ejemplo mÃ¡s que prueba lo poderosos que son estos embeddings es el concepto de las analogÃ­as. Podemos sumar y restar embeddings de palabras y llegar a resultados interesantes. El ejemplo mÃ¡s famoso es la fÃ³rmula: "rey" - "hombre" + "mujer":

![[king-analogy-viz.png|800]]

El vector resultante de "rey-hombre+mujer" no es exactamente igual a "reina", pero "reina" es la palabra mÃ¡s cercana de los 400 000 embeddings de palabras que tenemos en esta colecciÃ³n.

Algo importante que debe quedar claro es que, en contraste con los embeddings de personalidad que analizamos anteriormente, **_no sabemos que codifica cada dimensiÃ³n del vector_**. Podemos hacer conjeturas y experimentos que nos permitan validar que estos vectores estÃ¡n modelando de alguna u otra forma la semÃ¡ntica, pero esta codificaciÃ³n de los datos es la que a la red le pareciÃ³ la mÃ¡s eficiente durante el entrenamiento.


___

%%
tags:  #programaciÃ³n #pagmatpython #python  #embedding

VÃ­nculos:  [[000-MenÃº MatemÃ¡ticas Python ğŸ“ƒ|MenÃº MatemÃ¡ticas en pythonğŸ“ƒ]]
%%