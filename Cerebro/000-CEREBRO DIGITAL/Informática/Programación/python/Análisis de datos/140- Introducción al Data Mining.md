---
ID: 140
"tags:": 
"nombre:": Introducci칩n al Data Mining
---
___
# Introducci칩n al Data Mining
___
![](https://www.youtube.com/watch?v=US6uMyYJjRg&list=PL5C9QKu8AsmUK_7AEP0hSmt-8vcE8gnIB&index=24)

___
El Data Mining, tambi칠n conocido como miner칤a de datos, es el proceso de extracci칩n de patrones, conocimientos y correlaciones 칰tiles a partir de grandes vol칰menes de datos. Esto se logra mediante la aplicaci칩n de t칠cnicas de an치lisis de datos y algoritmos de aprendizaje autom치tico.


## Conceptos b치sicos de Data Mining

- **Dataset:** Conjunto de datos recolectados y organizados para su an치lisis.
- **Atributos**: Caracter칤sticas o variables que describen los objetos del dataset.
- **Registro**: Un conjunto de atributos que describen a un objeto en el dataset.
- **Clasificaci칩n**: Asignaci칩n de objetos a clases predefinidas en funci칩n de sus atributos.
- **Regresi칩n**: Predicci칩n de un valor num칠rico en funci칩n de los atributos de un objeto.
- **Agrupamiento**: Divisi칩n de los objetos en grupos seg칰n sus atributos, de manera que los objetos en un grupo sean similares entre s칤 y diferentes a los objetos en otros grupos.


## Proceso de Data Mining (CRISP-DM)

![[Pasted image 20230907203857.png]]

___
## 1- T칠cnica PCA

```python
pip install scikit-learn
```

La librer칤a `sklearn` en Python es una poderosa herramienta para realizar tareas de aprendizaje autom치tico y miner칤a de datos. En particular, `sklearn.decomposition` es un m칩dulo espec칤fico que se centra en t칠cnicas de reducci칩n de dimensionalidad, como el An치lisis de Componentes Principales (PCA, por sus siglas en ingl칠s).

El PCA es una t칠cnica que se utiliza para transformar un conjunto de datos en un nuevo sistema de coordenadas, donde las variables est치n des correlacionadas y ordenadas por su varianza. Esto significa que el PCA toma un conjunto de datos con muchas variables (o dimensiones) y lo proyecta en un espacio de menor dimensi칩n, manteniendo la mayor parte de la informaci칩n.

El objetivo principal de PCA es simplificar la representaci칩n de datos, manteniendo la estructura esencial de la informaci칩n. Esto puede ser 칰til para visualizar datos en dimensiones m치s bajas, reducir la complejidad computacional de los algoritmos de aprendizaje autom치tico y eliminar la multicolinealidad entre variables.

```python
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
  
from sklearn.decomposition import PCA
```

Primero, se importan las bibliotecas `seaborn` (que generalmente se usa para visualizaci칩n de datos), `PCA` de `sklearn` (para realizar el An치lisis de Componentes Principales) y `matplotlib.pyplot` (para crear gr치ficos).

```python
df = sns.load_dataset("iris")
df
```
$$Consola$$
![[Pasted image 20230908223551.png]]

Se carga el conjunto de datos "iris" utilizando la funci칩n `load_dataset` de `seaborn`. Este conjunto de datos es famoso y se utiliza com칰nmente para tareas de clasificaci칩n en aprendizaje autom치tico. Contiene informaci칩n sobre tres especies diferentes de flores iris y las medidas de sus s칠palos y p칠talos.

```python
pca_model = PCA(n_components=2)
df_pca = pca_model.fit_transform(df.iloc[:,:-1])
df_pca
```
$$Consola$$
![[Pasted image 20230908224102.png]]


1. Se crea un modelo de An치lisis de Componentes Principales (PCA) con `n_components=2`, lo que significa que queremos reducir la dimensionalidad del conjunto de datos original a solo 2 componentes principales. Esto simplificar치 los datos y permitir치 una visualizaci칩n m치s f치cil.

2. Se aplica PCA al conjunto de datos original `df`. `df.iloc[:,:-1]` se utiliza para seleccionar todas las columnas excepto la 칰ltima, que contiene la informaci칩n de la especie de la flor. El resultado se almacena en `df_pca`, que ahora contiene las coordenadas de los datos en el espacio de los dos componentes principales.
   1. **Fit (Ajustar)**: En la primera parte de la operaci칩n, el algoritmo o modelo se "ajusta" a los datos de entrada. En el contexto de PCA, esto significa que el algoritmo analiza los datos para calcular los componentes principales y otras estad칤sticas necesarias. El ajuste implica encontrar patrones, relaciones y caracter칤sticas importantes en los datos de entrada.
   2. **Transform (Transformar)**: En la segunda parte de la operaci칩n, los datos de entrada se transforman utilizando la informaci칩n recopilada durante la fase de ajuste. En el caso de PCA, se realizan las transformaciones lineales necesarias para proyectar los datos originales en un nuevo espacio de caracter칤sticas definido por los componentes principales.

```python
plt.scatter(df_pca[:,0], df_pca[:,1], c=df["species"].astype("category").cat.codes)
plt.xlabel("PCA1")
plt.ylabel("PCA2")
plt.show
```
$$Consola$$
![[Pasted image 20230908224142.png]]


1. Se crea un gr치fico de dispersi칩n (`scatter plot`) utilizando los valores de los componentes principales. `df_pca[:,0]` y `df_pca[:,1]` se utilizan como las coordenadas `x` e `y` respectivamente, y se utiliza el color `c` para codificar las especies de iris. Las especies se codifican como n칰meros enteros utilizando `cat.codes`.

2. Se a침aden etiquetas a los ejes `x` e `y` del gr치fico para indicar qu칠 componentes principales representan.

3. Finalmente, se muestra el gr치fico de dispersi칩n que visualiza los datos en el espacio de los dos primeros componentes principales. Esto permitir치 ver si las muestras de diferentes especies de iris se agrupan o separan en funci칩n de estos componentes principales.


## 2- T칠cnica Transformaci칩n de Variables
La t칠cnica de "transformaci칩n de variables" se refiere al proceso de modificar las variables de un conjunto de datos para mejorar su distribuci칩n, escalado o relaci칩n con otras variables. Esto se hace con el objetivo de preparar los datos para su uso en modelos de aprendizaje autom치tico o para an치lisis estad칤sticos m치s efectivos.

Aqu칤 te explicar칠 algunas t칠cnicas comunes de transformaci칩n de variables:

1. **Normalizaci칩n o Escalado (Feature Scaling):** Esta t칠cnica implica ajustar el rango de valores de las variables para que todos est칠n en una escala similar. La normalizaci칩n es especialmente importante para algoritmos sensibles a la escala, como el descenso de gradiente en el aprendizaje de m치quinas. Los m칠todos comunes incluyen la estandarizaci칩n (restar la media y dividir por la desviaci칩n est치ndar) y la escala m칤n-m치x (reescalar los valores al rango [0, 1]).
    
2. **Transformaci칩n Logar칤tmica:** Se aplica el logaritmo natural a los valores de una variable. Esto es 칰til cuando la distribuci칩n de la variable original es sesgada hacia la derecha, ya que la transformaci칩n logar칤tmica puede reducir este sesgo.
    
3. **Transformaci칩n de Ra칤z Cuadrada:** Se toma la ra칤z cuadrada de los valores de una variable. Al igual que la transformaci칩n logar칤tmica, esta t칠cnica se utiliza para reducir el sesgo en una distribuci칩n sesgada hacia la derecha.
    
4. **Transformaci칩n de Box-Cox:** Es una familia de transformaciones que incluye tanto la transformaci칩n logar칤tmica como la ra칤z cuadrada como casos especiales. La transformaci칩n de Box-Cox es 칰til cuando no est치 claro cu치l de las transformaciones anteriores ser칤a la mejor.
    
5. **Discretizaci칩n:** Convierte variables continuas en variables categ칩ricas o discretas. Esto puede simplificar la interpretaci칩n o ser necesario para ciertos algoritmos de aprendizaje autom치tico.
    
6. **One-Hot Encoding:** Convierte variables categ칩ricas en una representaci칩n binaria. Esto es 칰til para modelos que requieren variables num칠ricas, como regresi칩n lineal o redes neuronales.
    
7. **Binning o Agrupaci칩n:** Divide una variable continua en intervalos (bins) y asigna cada valor a uno de esos intervalos. Esto puede simplificar la relaci칩n entre la variable y el objetivo.
    
8. **Funciones de Base:** Se utilizan en regresiones polin칩micas o regresiones no lineales para transformar las variables predictoras en funciones m치s complejas.
    
9. **Transformaciones de Escala Robusta:** Algunas t칠cnicas, como la Escala Robusta, son menos sensibles a los valores at칤picos y pueden ser m치s apropiadas para conjuntos de datos con presencia de outliers.

```python
sns.displot(data=df, x = "sepal_length", hue="species", kde=True)
plt.show()
```
$$Consola$$
![[Pasted image 20230908225518.png]]

### Transformaci칩n logar칤tmica

```python
df["sepal_length_log"] = np.log(df["sepal_length"])
  
sns.displot(data=df, x = "sepal_length_log", hue="species", kde=True)
plt.show()
```
$$Consola$$
![[Pasted image 20230908225556.png]]


## 3- T칠cnica de Normalizaci칩n de datos

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler
```

``StandardScaler()``
Su funci칩n principal es realizar una transformaci칩n en los datos para que tengan una media (promedio) de cero y una desviaci칩n est치ndar de uno. Esto se conoce como estandarizaci칩n o escalado est치ndar.

La estandarizaci칩n es una t칠cnica com칰n en el preprocesamiento de datos y es 칰til cuando se trabaja con algoritmos de aprendizaje autom치tico que son sensibles a la escala de las caracter칤sticas (como la regresi칩n lineal, la regresi칩n log칤stica y las m치quinas de soporte vectorial).
```python
std_scaler = StandardScaler()
df_std_scaler = std_scaler.fit_transform(df.iloc[:,:-2])
df_std_scaler
```
$$Consola$$
![[Pasted image 20230909195347.png]]


Mientras que `StandardScaler()` estandariza los datos para que tengan una media de cero y una desviaci칩n est치ndar de uno, `MinMaxScaler()` realiza una transformaci칩n que ajusta los datos a un rango espec칤fico, generalmente entre 0 y 1.
```python
mm_scaler = MinMaxScaler()
df_normalized = mm_scaler.fit_transform(df.iloc[:,:-2])
df_normalized
```
$$Consola$$
![[Pasted image 20230909195722.png]]


## 4- T칠cnica de Correlaci칩n (Pearson)

Es una t칠cnica estad칤stica que se utiliza para medir la relaci칩n lineal entre dos variables continuas. En otras palabras, determina cu치n estrechamente relacionadas est치n dos variables y en qu칠 direcci칩n se relacionan.

El coeficiente de correlaci칩n de Pearson puede tomar valores en el rango de -1 a 1:

- Si el coeficiente es 1, indica una correlaci칩n positiva perfecta, lo que significa que a medida que una variable aumenta, la otra tambi칠n aumenta en una relaci칩n lineal perfecta.
- Si el coeficiente es -1, indica una correlaci칩n negativa perfecta, lo que significa que a medida que una variable aumenta, la otra disminuye en una relaci칩n lineal perfecta.
- Si el coeficiente es 0, indica que no hay correlaci칩n lineal entre las dos variables. En otras palabras, no hay una relaci칩n lineal predecible entre ellas.

Algunos puntos clave sobre la correlaci칩n de Pearson:

- Mide solo relaciones lineales. Si la relaci칩n entre las variables es no lineal, el coeficiente de correlaci칩n de Pearson no capturar치 esa relaci칩n.
- Es sensible a los valores at칤picos (outliers). Los valores at칤picos pueden afectar significativamente el valor de la correlaci칩n de Pearson.
- No implica causalidad. Una alta correlaci칩n entre dos variables no implica necesariamente que una variable cause la otra. La correlaci칩n no determina la direcci칩n de la causalidad.

```python
pip install scipy
```



```python
from scipy.stats import pearsonr
```

***Calcular la correlaci칩n de pearson***
```python
correlacion, _ = pearsonr(df["sepal_length"], df["petal_length"])
print(f"La correlaci칩n de pearson entre sepal length y petal length es: {correlacion}")
```
$$Consola$$
![[Pasted image 20230909200727.png]]

## 5- T칠cnica An치lisis de Clusters

El An치lisis de Clusters, tambi칠n conocido como Agrupamiento o Clustering en ingl칠s, es una t칠cnica de aprendizaje no supervisado en el campo del aprendizaje autom치tico y la estad칤stica. Su objetivo principal es dividir un conjunto de datos en grupos o cl칰steres, de modo que los elementos dentro de un cl칰ster sean m치s similares entre s칤 que con los elementos de otros cl칰steres. En otras palabras, el an치lisis de clusters busca descubrir patrones naturales o estructuras intr칤nsecas en los datos.

```python
from sklearn.cluster import KMeans
```



```python
model = KMeans(n_clusters=3, random_state=42)
df["cluster"] = model.fit_predict(df.iloc[:,:-2])
df
```
$$Consola$$
![[Pasted image 20230909201404.png]]




```python
sns.scatterplot(data=df, x="sepal_length", y="petal_length", hue="cluster")
plt.show()
```
$$Consola$$
![[Pasted image 20230909201421.png]]


## 6-  T칠cnica Z-Score
El Z-Score, tambi칠n conocido como puntaje Z, es una t칠cnica estad칤stica utilizada para evaluar cu치n lejos de la media se encuentra un valor en una distribuci칩n de datos y cu치ntas desviaciones est치ndar est치 por encima o por debajo de la media. El Z-Score se utiliza com칰nmente para identificar valores at칤picos (outliers) en un conjunto de datos.

***CALCULAR EL Z-SCORE***
```python
df["z_score_sepal_length"] = np.abs((df["sepal_length"] - df["sepal_length"].mean())/df["sepal_length"].std())
df
```
$$Consola$$
![[Pasted image 20230909202534.png]]

```python
outliers = df[df["z_score_sepal_length"] > 2]
outliers
```
$$Consola$$
![[Pasted image 20230909202609.png]]

```python
sns.scatterplot(data = df, x="sepal_length", y="petal_length", hue="species")
sns.scatterplot(data = outliers, x="sepal_length", y="petal_length", c="red")
```
$$Consola$$
![[Pasted image 20230909202636.png]]






___
%%
tags: #programaci칩n #pagan치lisis_de_datos #python  #an치lisis_de_datos

V칤nculos:   [[000-Men칰 An치lisis de datos 游늮|Men칰 An치lisis de datos 游늮]] 
%%